{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795dbefb",
   "metadata": {},
   "source": [
    "### Build A simple LLM Application with LCEL\n",
    "\n",
    "This application will translate text from english into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf7764",
   "metadata": {},
   "source": [
    "What is LCEL?\n",
    "\n",
    "LCEL stands for LangChain Expression Language.\n",
    "\n",
    "It is a special language (or syntax) used inside LangChain to build AI pipelines step-by-step in a clean, readable, and modular way.\n",
    "\n",
    "Think of it as a tool that lets you connect different AI components like Lego blocks.\n",
    "\n",
    "Example:\n",
    "\n",
    "Load text\n",
    "\n",
    "Split into chunks\n",
    "\n",
    "Add embeddings\n",
    "\n",
    "Search\n",
    "\n",
    "Ask an LLM\n",
    "\n",
    "Format the output\n",
    "\n",
    "LCEL lets you combine these steps neatly.\n",
    "\n",
    "âœ… Why is LCEL used?\n",
    "\n",
    "Without LCEL, building AI pipelines becomes:\n",
    "\n",
    "messy\n",
    "\n",
    "long\n",
    "\n",
    "full of repeated code\n",
    "\n",
    "harder to debug\n",
    "\n",
    "LCEL solves this by letting you:\n",
    "\n",
    "âœ” Connect steps like a flow\n",
    "âœ” Keep code short and readable\n",
    "âœ” Run pipelines sync or async\n",
    "âœ” Easily swap components (LLM, embedding model, vector store)\n",
    "âœ” Reuse the same pipeline for multiple tasks\n",
    "\n",
    "It is basically clean code for AI workflows.\n",
    "\n",
    "âœ… Where is LCEL used?\n",
    "\n",
    "LCEL is used in LangChain-based AI applications, such as:\n",
    "\n",
    "ğŸ”¹ RAG (Retrieval-Augmented Generation) systems\n",
    "\n",
    "â€“ search documents â†’ feed to LLM â†’ format answer\n",
    "\n",
    "ğŸ”¹ Chatbots\n",
    "\n",
    "â€“ detect intent â†’ generate response â†’ store memory\n",
    "\n",
    "ğŸ”¹ Agent workflows\n",
    "\n",
    "â€“ define tools â†’ route decisions â†’ call LLM â†’ return results\n",
    "\n",
    "ğŸ”¹ Knowledge-base apps\n",
    "\n",
    "â€“ load PDFs â†’ embed text â†’ retrieve relevant info\n",
    "\n",
    "ğŸ”¹ LLM tool pipelines\n",
    "\n",
    "â€“ parse input â†’ run multiple tools â†’ combine results\n",
    "\n",
    "Essentially, everywhere you want to structure data flow between AI components.\n",
    "\n",
    "âœ… When is LCEL used?\n",
    "\n",
    "You use LCEL when:\n",
    "\n",
    "âœ” You want to chain multiple steps together\n",
    "\n",
    "(example: loader â†’ splitter â†’ embeddings â†’ vector search â†’ LLM)\n",
    "\n",
    "âœ” You want a clean, readable pipeline\n",
    "\n",
    "Not scattered code.\n",
    "\n",
    "âœ” You want reusability\n",
    "\n",
    "One pipeline can be reused for many queries.\n",
    "\n",
    "âœ” You want to build production-ready AI apps\n",
    "\n",
    "Because LCEL supports streaming, batching, async, and parallel execution.\n",
    "\n",
    "âœ” You want to modify pipelines easily\n",
    "\n",
    "Just replace one block (like swapping OpenAI with Llama).\n",
    "\n",
    "ğŸ¯ Simple Analogy\n",
    "\n",
    "Imagine making tea:\n",
    "\n",
    "Boil water\n",
    "\n",
    "Add tea leaves\n",
    "\n",
    "Add sugar\n",
    "\n",
    "Add milk\n",
    "\n",
    "Strain\n",
    "\n",
    "Serve\n",
    "\n",
    "LCEL lets you write these steps as:\n",
    "\n",
    "boil | addTea | addSugar | addMilk | strain | serve\n",
    "\n",
    "\n",
    "Instead of writing a long, complicated function.\n",
    "\n",
    "ğŸ§  In summary\n",
    "Question\tAnswer\n",
    "What is LCEL?\tA syntax in LangChain to connect AI steps like a pipeline.\n",
    "Why used?\tMakes AI workflows clean, modular, fast, and reusable.\n",
    "Where used?\tRAG systems, chatbots, agents, embedding pipelines, tool workflows.\n",
    "When used?\tWhenever multiple AI steps need to be combined in a structured way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f357a30",
   "metadata": {},
   "source": [
    "ğŸš€ What is Groq LPU?\n",
    "\n",
    "Groq LPU stands for Language Processing Unit.\n",
    "\n",
    "It is a special type of processor (hardware chip) created by Groq to run AI models extremely fast â€” especially large language models (LLMs).\n",
    "\n",
    "Think of an LPU as something similar to:\n",
    "\n",
    "CPU â†’ Handles general tasks\n",
    "\n",
    "GPU â†’ Handles graphics + AI tasks\n",
    "\n",
    "LPU â†’ Handles only language AI tasks at very high speed\n",
    "\n",
    "ğŸ¤” Why is it called LPU?\n",
    "\n",
    "Because it is designed specifically for language processing, such as:\n",
    "\n",
    "text generation\n",
    "\n",
    "embeddings\n",
    "\n",
    "transformers\n",
    "\n",
    "chat models\n",
    "\n",
    "Rather than being general-purpose like CPU or GPU, an LPU is purpose-built for LLM inference.\n",
    "\n",
    "â­ Why is the Groq LPU used? (Benefits)\n",
    "\n",
    "Groq LPUs are used because they offer:\n",
    "\n",
    "âœ” Insane speed (token generation speed ~300 tokens/sec per user)\n",
    "\n",
    "Much faster than GPUs for many LLM tasks.\n",
    "\n",
    "âœ” Low latency\n",
    "\n",
    "Responses start almost instantly.\n",
    "\n",
    "âœ” High parallelism\n",
    "\n",
    "Can handle many users at the same time.\n",
    "\n",
    "âœ” Energy efficiency\n",
    "\n",
    "Consumes less power than GPUs for the same tasks.\n",
    "\n",
    "âœ” Cheaper to run\n",
    "\n",
    "Lower cost per token generated.\n",
    "\n",
    "In short â€” fast, cheap, efficient AI inference.\n",
    "\n",
    "ğŸ“ Where is Groq LPU used?\n",
    "\n",
    "Groq LPUs are used in:\n",
    "\n",
    "ğŸ”¹ AI Applications\n",
    "\n",
    "Chatbots\n",
    "\n",
    "Customer support agents\n",
    "\n",
    "LLM APIs\n",
    "\n",
    "RAG-based search\n",
    "\n",
    "Code assistants\n",
    "\n",
    "ğŸ”¹ Cloud Providers\n",
    "\n",
    "Groq offers its own cloud (GroqCloud), where you can run models like:\n",
    "\n",
    "Llama 3\n",
    "\n",
    "Mixtral\n",
    "\n",
    "Gemma\n",
    "\n",
    "DeepSeek LLM\n",
    "\n",
    "ğŸ”¹ Enterprise AI systems\n",
    "\n",
    "For companies needing large-scale:\n",
    "\n",
    "text processing\n",
    "\n",
    "document summarization\n",
    "\n",
    "automation systems\n",
    "\n",
    "ğŸ”¹ High-speed inference environments\n",
    "\n",
    "Where low-latency is critical.\n",
    "\n",
    "â° When is Groq LPU used?\n",
    "\n",
    "You should use an LPU when:\n",
    "\n",
    "ğŸš€ You need very fast AI responses\n",
    "\n",
    "(e.g., live chat, coding apps, agents)\n",
    "\n",
    "ğŸ§  You want to serve many users at once\n",
    "ğŸ’¸ You want lower cost compared to GPU-based inference\n",
    "ğŸ”Œ You need efficient hardware that uses less power\n",
    "ğŸ§© You are running models optimized for Groq\n",
    "\n",
    "(e.g., Llama 3, Mistral models)\n",
    "\n",
    "ğŸ§  Simple Analogy\n",
    "\n",
    "If CPU is a bike,\n",
    "and GPU is a sports car,\n",
    "then Groq LPU is a Formula-1 race car built ONLY for one purpose:\n",
    "running language models at extreme speed.\n",
    "\n",
    "âœ” In Summary\n",
    "Question\tSimple Answer\n",
    "What is Groq LPU?\tA special AI processor built by Groq for super-fast language model inference.\n",
    "Why used?\tVery high speed, low latency, energy-efficient, and cheaper than GPUs.\n",
    "Where used?\tAI apps, inference clouds, chatbots, RAG systems, enterprise automation.\n",
    "When used?\tWhen you need fast, scalable, low-cost AI text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31ef2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import langchain_openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c19b6646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000260873A44D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000026087512990>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
