{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795dbefb",
   "metadata": {},
   "source": [
    "### Build A simple LLM Application with LCEL\n",
    "\n",
    "This application will translate text from english into another language. This is a relatively simple LLM application - it's just a single LLM call plus some prompting. Still, this is a great way to get started with LangChain - a lot of features can be built with just some prompting and an LLM call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf7764",
   "metadata": {},
   "source": [
    "What is LCEL?\n",
    "\n",
    "LCEL stands for LangChain Expression Language.\n",
    "\n",
    "It is a special language (or syntax) used inside LangChain to build AI pipelines step-by-step in a clean, readable, and modular way.\n",
    "\n",
    "Think of it as a tool that lets you connect different AI components like Lego blocks.\n",
    "\n",
    "Example:\n",
    "\n",
    "Load text\n",
    "\n",
    "Split into chunks\n",
    "\n",
    "Add embeddings\n",
    "\n",
    "Search\n",
    "\n",
    "Ask an LLM\n",
    "\n",
    "Format the output\n",
    "\n",
    "LCEL lets you combine these steps neatly.\n",
    "\n",
    "‚úÖ Why is LCEL used?\n",
    "\n",
    "Without LCEL, building AI pipelines becomes:\n",
    "\n",
    "messy\n",
    "\n",
    "long\n",
    "\n",
    "full of repeated code\n",
    "\n",
    "harder to debug\n",
    "\n",
    "LCEL solves this by letting you:\n",
    "\n",
    "‚úî Connect steps like a flow\n",
    "‚úî Keep code short and readable\n",
    "‚úî Run pipelines sync or async\n",
    "‚úî Easily swap components (LLM, embedding model, vector store)\n",
    "‚úî Reuse the same pipeline for multiple tasks\n",
    "\n",
    "It is basically clean code for AI workflows.\n",
    "\n",
    "‚úÖ Where is LCEL used?\n",
    "\n",
    "LCEL is used in LangChain-based AI applications, such as:\n",
    "\n",
    "üîπ RAG (Retrieval-Augmented Generation) systems\n",
    "\n",
    "‚Äì search documents ‚Üí feed to LLM ‚Üí format answer\n",
    "\n",
    "üîπ Chatbots\n",
    "\n",
    "‚Äì detect intent ‚Üí generate response ‚Üí store memory\n",
    "\n",
    "üîπ Agent workflows\n",
    "\n",
    "‚Äì define tools ‚Üí route decisions ‚Üí call LLM ‚Üí return results\n",
    "\n",
    "üîπ Knowledge-base apps\n",
    "\n",
    "‚Äì load PDFs ‚Üí embed text ‚Üí retrieve relevant info\n",
    "\n",
    "üîπ LLM tool pipelines\n",
    "\n",
    "‚Äì parse input ‚Üí run multiple tools ‚Üí combine results\n",
    "\n",
    "Essentially, everywhere you want to structure data flow between AI components.\n",
    "\n",
    "‚úÖ When is LCEL used?\n",
    "\n",
    "You use LCEL when:\n",
    "\n",
    "‚úî You want to chain multiple steps together\n",
    "\n",
    "(example: loader ‚Üí splitter ‚Üí embeddings ‚Üí vector search ‚Üí LLM)\n",
    "\n",
    "‚úî You want a clean, readable pipeline\n",
    "\n",
    "Not scattered code.\n",
    "\n",
    "‚úî You want reusability\n",
    "\n",
    "One pipeline can be reused for many queries.\n",
    "\n",
    "‚úî You want to build production-ready AI apps\n",
    "\n",
    "Because LCEL supports streaming, batching, async, and parallel execution.\n",
    "\n",
    "‚úî You want to modify pipelines easily\n",
    "\n",
    "Just replace one block (like swapping OpenAI with Llama).\n",
    "\n",
    "üéØ Simple Analogy\n",
    "\n",
    "Imagine making tea:\n",
    "\n",
    "Boil water\n",
    "\n",
    "Add tea leaves\n",
    "\n",
    "Add sugar\n",
    "\n",
    "Add milk\n",
    "\n",
    "Strain\n",
    "\n",
    "Serve\n",
    "\n",
    "LCEL lets you write these steps as:\n",
    "\n",
    "boil | addTea | addSugar | addMilk | strain | serve\n",
    "\n",
    "\n",
    "Instead of writing a long, complicated function.\n",
    "\n",
    "üß† In summary\n",
    "Question\tAnswer\n",
    "What is LCEL?\tA syntax in LangChain to connect AI steps like a pipeline.\n",
    "Why used?\tMakes AI workflows clean, modular, fast, and reusable.\n",
    "Where used?\tRAG systems, chatbots, agents, embedding pipelines, tool workflows.\n",
    "When used?\tWhenever multiple AI steps need to be combined in a structured way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f357a30",
   "metadata": {},
   "source": [
    "üöÄ What is Groq LPU?\n",
    "\n",
    "Groq LPU stands for Language Processing Unit.\n",
    "\n",
    "It is a special type of processor (hardware chip) created by Groq to run AI models extremely fast ‚Äî especially large language models (LLMs).\n",
    "\n",
    "Think of an LPU as something similar to:\n",
    "\n",
    "CPU ‚Üí Handles general tasks\n",
    "\n",
    "GPU ‚Üí Handles graphics + AI tasks\n",
    "\n",
    "LPU ‚Üí Handles only language AI tasks at very high speed\n",
    "\n",
    "ü§î Why is it called LPU?\n",
    "\n",
    "Because it is designed specifically for language processing, such as:\n",
    "\n",
    "text generation\n",
    "\n",
    "embeddings\n",
    "\n",
    "transformers\n",
    "\n",
    "chat models\n",
    "\n",
    "Rather than being general-purpose like CPU or GPU, an LPU is purpose-built for LLM inference.\n",
    "\n",
    "‚≠ê Why is the Groq LPU used? (Benefits)\n",
    "\n",
    "Groq LPUs are used because they offer:\n",
    "\n",
    "‚úî Insane speed (token generation speed ~300 tokens/sec per user)\n",
    "\n",
    "Much faster than GPUs for many LLM tasks.\n",
    "\n",
    "‚úî Low latency\n",
    "\n",
    "Responses start almost instantly.\n",
    "\n",
    "‚úî High parallelism\n",
    "\n",
    "Can handle many users at the same time.\n",
    "\n",
    "‚úî Energy efficiency\n",
    "\n",
    "Consumes less power than GPUs for the same tasks.\n",
    "\n",
    "‚úî Cheaper to run\n",
    "\n",
    "Lower cost per token generated.\n",
    "\n",
    "In short ‚Äî fast, cheap, efficient AI inference.\n",
    "\n",
    "üìç Where is Groq LPU used?\n",
    "\n",
    "Groq LPUs are used in:\n",
    "\n",
    "üîπ AI Applications\n",
    "\n",
    "Chatbots\n",
    "\n",
    "Customer support agents\n",
    "\n",
    "LLM APIs\n",
    "\n",
    "RAG-based search\n",
    "\n",
    "Code assistants\n",
    "\n",
    "üîπ Cloud Providers\n",
    "\n",
    "Groq offers its own cloud (GroqCloud), where you can run models like:\n",
    "\n",
    "Llama 3\n",
    "\n",
    "Mixtral\n",
    "\n",
    "Gemma\n",
    "\n",
    "DeepSeek LLM\n",
    "\n",
    "üîπ Enterprise AI systems\n",
    "\n",
    "For companies needing large-scale:\n",
    "\n",
    "text processing\n",
    "\n",
    "document summarization\n",
    "\n",
    "automation systems\n",
    "\n",
    "üîπ High-speed inference environments\n",
    "\n",
    "Where low-latency is critical.\n",
    "\n",
    "‚è∞ When is Groq LPU used?\n",
    "\n",
    "You should use an LPU when:\n",
    "\n",
    "üöÄ You need very fast AI responses\n",
    "\n",
    "(e.g., live chat, coding apps, agents)\n",
    "\n",
    "üß† You want to serve many users at once\n",
    "üí∏ You want lower cost compared to GPU-based inference\n",
    "üîå You need efficient hardware that uses less power\n",
    "üß© You are running models optimized for Groq\n",
    "\n",
    "(e.g., Llama 3, Mistral models)\n",
    "\n",
    "üß† Simple Analogy\n",
    "\n",
    "If CPU is a bike,\n",
    "and GPU is a sports car,\n",
    "then Groq LPU is a Formula-1 race car built ONLY for one purpose:\n",
    "running language models at extreme speed.\n",
    "\n",
    "‚úî In Summary\n",
    "Question\tSimple Answer\n",
    "What is Groq LPU?\tA special AI processor built by Groq for super-fast language model inference.\n",
    "Why used?\tVery high speed, low latency, energy-efficient, and cheaper than GPUs.\n",
    "Where used?\tAI apps, inference clouds, chatbots, RAG systems, enterprise automation.\n",
    "When used?\tWhen you need fast, scalable, low-cost AI text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f31ef2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dtp202505-u05/Desktop/langchain_agenticAI/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import langchain_openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c19b6646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x783fdf4b4490>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x783fdf1a3c90>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.1-8b-instant\", groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a51a65c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the follwoing sentence into French', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi you are very good', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "message = [\n",
    "    SystemMessage(content=\"Translate the follwoing sentence into French\"),\n",
    "    HumanMessage(content=\"Hi you are very good\")\n",
    "]\n",
    "\n",
    "message "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fd6965d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Bonjour, vous √™tes tr√®s bon.\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "response = parser.invoke(model.invoke(message))\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b498444c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour, vous √™tes tr√®s bon.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = model|parser\n",
    "chain.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35ac62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following sentence in to French:', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello you are very good', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "generaic_templae = \"Translate the following sentence in to {language}:\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",generaic_templae),(\"user\",\"{text}\")\n",
    "])\n",
    "\n",
    "prompt.invoke({\"language\":\"French\", \"text\":\"Hello you are very good\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70f68f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour, vous √™tes un tr√®s bon gar√ßon. \\n\\n(Note: In French, the subject \"you\" is typically addressed with \"vous\" for formal situations, and \"tu\" for informal situations. I used \"vous\" in this translation as it is a more general polite form. If you want to use the informal form, the translation would be: Salut, tu es un tr√®s bon gar√ßon.)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt|model|parser\n",
    "chain.invoke({\"language\":\"French\", \"text\":\"Hello you are very good boy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629f4eb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
